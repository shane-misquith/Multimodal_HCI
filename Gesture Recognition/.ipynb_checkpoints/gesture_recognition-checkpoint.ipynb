{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "turned-syndrome",
   "metadata": {},
   "source": [
    "# Final Year Project Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stuck-planner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import autopy\n",
    "\n",
    "w_cam, h_cam = 640, 480 # setting width and height of webcam\n",
    "w_screen, h_screen = autopy.screen.size() # height and width of screen\n",
    "frame = 100 # width and height of the frame inside the webcam window\n",
    "smoothening = 7\n",
    "prev_loc_x = prev_loc_y = 0\n",
    "current_loc_x = current_loc_y = 0\n",
    "\n",
    "cap = cv2.VideoCapture(0) # 0 will take the input from the default camera. 1, 2 etc id's for other cameras\n",
    "cap.set(3, w_cam) # width (id is 3)\n",
    "cap.set(4, h_cam) # height (id is 4)\n",
    "cap.set(10, 100) # brightness (id is 10)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "# hands = mp_hands.Hands() # default parameters are preferred\n",
    "hands = mp_hands.Hands(max_num_hands = 1)\n",
    "mp_draw = mp.solutions.drawing_utils # function to draw (visualize) line and points which was used to detect hands\n",
    "\n",
    "# used for calculating FPS\n",
    "previous_time = 0\n",
    "current_time = 0\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Hands class only uses RGB images\n",
    "    results = hands.process(img_rgb)\n",
    "#     print(results.multi_hand_landmarks) # will give co-ordinates if hands are detected, else None\n",
    "\n",
    "    if results.multi_hand_landmarks: # if hands are detected\n",
    "#         for hand_lms in results.multi_hand_landmarks: # for each hand detected(max 2 hands by default - Hands())\n",
    "        hand_lms = results.multi_hand_landmarks[0] # first hand (here, only one hand is detected anyway)\n",
    "#         lm - coordinate of point in ratio of h(x),w(y) and z. \n",
    "#         These landmarks will always be in order and hence we get id by enumerate\n",
    "        lm = hand_lms.landmark[9] # total of 21 points(0 - 20 id's)\n",
    "\n",
    "        h, w, c = img.shape # height, width and channels of the img(frame)\n",
    "        cix, ciy = int(lm.x * w), int(lm.y * h) # coordinates with respect to the pixels of the img(frame)           \n",
    "\n",
    "        cv2.circle(img, (cix, ciy), 15, (255,0,255), cv2.FILLED) # highlighting the specific landmark\n",
    "\n",
    "        # frame where hand movement is detected\n",
    "        cv2.rectangle(img, (frame, frame), (w_cam-frame, h_cam-frame), (255, 0, 255), 2)\n",
    "\n",
    "#         csx = np.interp(cix, (0, w_cam), (0, w_screen)) # range from 0 to width of webcam is converted to 0 to width of screen\n",
    "#         csy = np.interp(ciy, (0, h_cam), (0, h_screen)) # range from 0 to height of webcam is converted to 0 to height of screen\n",
    "        csx = np.interp(cix, (frame, w_cam-frame), (0, w_screen)) # range from 0 to width of frame is converted to 0 to width of screen\n",
    "        csy = np.interp(ciy, (frame, h_cam-frame), (0, h_screen)) # range from 0 to height of frame is converted to 0 to height of screen\n",
    "#         print(csx, csy)\n",
    "\n",
    "#         print(\"Landmark: [{0}, {1}]\".format(lm.x, lm.y))\n",
    "#         print(\"Webcam frame coordinates: [{0}, {1}]\".format(cix, ciy))\n",
    "#         print(\"Screen coordinates: [{0}, {1}]\\n\".format(csx, csy))\n",
    "#         Smootheing x and y value\n",
    "        current_loc_x = prev_loc_x + (csx - prev_loc_x) / smoothening\n",
    "        current_loc_y = prev_loc_y + (csy - prev_loc_y) / smoothening\n",
    "\n",
    "#         giving mouse coordinates(x coordinate is inverted, hence we subtract it from width of screen)\n",
    "        autopy.mouse.move(w_screen - current_loc_x, current_loc_y)\n",
    "\n",
    "#         img - destination image, hand_lms - for each hand, mp_hands.HAND_CONNECTIONS - to connect the dots(points)\n",
    "        mp_draw.draw_landmarks(img, hand_lms, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        prev_loc_x, prev_loc_y = current_loc_x, current_loc_y\n",
    "    \n",
    "    current_time = time.time()\n",
    "    fps = 1 / (current_time - previous_time)\n",
    "    previous_time = time.time()\n",
    "    \n",
    "    cv2.putText(img, str(int(fps)), (10,70), cv2.FONT_HERSHEY_COMPLEX, 2, (0,0,255), 3)\n",
    "    cv2.imshow(\"Video\", img)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): # adds a delay between each image and checks if 'q' is pressed to close the window\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows() # for jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-gibson",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
